---
Name: Siddharth Nachaloor
Topic: 1D unconstrained optimization algorithms. Parabolic interpolation, Newton's method, etc. How they work, rates of convergence, why they work, pros and cons of the various different methods.
Title: A Closer Look at How Optimization Methods Work
---
# Optimization

## Table of Contents
- [Overview](#Overview)
- [Background](#Background)
- [Penalty Function Method Overview](#Penalty-Function-Method-Overview)
- [Common Applications](#Common-Applications)
- [Formulation](#Formulation)
- [Penalty Function Options](#Penalty-Function-Options)
- [References](#References)

## What is Optimization?
XXXXXXXXX

## 1D Unconstrained Optimization and Parabolic Interpolation
XXXX

$$
\begin{align}
minimize \ : \ f(x) \\
subject \ to: \ g(x) = 0 , \ h(x) \leq 0 \\
\end{align}
$$

where f(x) XXXX


XXXXXXX
![](general_iteration.PNG)

**Newton's Method** [1]
1. XXXX
2. XXXX
3. XXXX
4. XXXX
5. XXXX

XXXXXXX
![](global_local.PNG)

XXXXX


## Other algorithms for Unconstrained Optimization (Steepest Descent Method, Secant Method, etc.)
XXXX included in the [Formulation](#Formulation) Section.

XXXX is shown below [2]:

![](exterior_vs_interior.PNG)

In the BLANK method, XXXX a necessary feature of the solution method [2].

## Real-life Applications
XXXXX

XXX used in combination with [genetic algorithms](https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3) to XXXX and robustness [9,10].

## Another Algorithm
XXXX

$$
\begin{align}
\phi(x,r) = f(x) + P(h(x),g(x),r) \\
\end{align}
$$

where XXXX $f(x)$ value.

**Other Algorithm** [1]
1. Start with an initial design point, **$x_0$**
2. Set the initial penalty parameter, **$p_0$**
3. Determine which constraints are violated given the current design variable values
4. Construct **$\phi(x,r)$** using the current design variables and penalty parameter
5. Find search direction as **$d_i = \nabla \phi(x,r)$**
6. Express the new design variables in terms of step length (still unknown) and search direction: $x_{i+1} = x_i + \alpha * d_i$
7. Substitute this expression into **$\phi(x,r)$** and find a step size to minimize phi using any unconstrained optimization method
8. Substitute the found step size into the expression for $x_i$ to find new design variable values
9. Update the penalty parameter as $r_{k+1} = C * r_i$ where C is a constant
10. Repeat until optimum is reached

The above iterative process ends when none of the constraints are violated. As a result, this method doesnâ€™t necessarily find interior extremes, but rather just extremes that occur on the boundary between the feasible and infeasible regions. Whether the algorithm converges on a result (and how fast this convergence occurs) is dependent on the choice of penalty function. A comparison of choices for penalty functions and more detail on steps three and four are given in the following section, [Penalty Function Options](#Penalty-Function-Options).

This process is often put into practice in code, as several iterations are necessary to reach the feasible region:

```
$$
x_i = x_0
p_i = C*f(x_i)				{Where C is between 1.1 and 2.0)
while h(x_i) >= 0, g(x_i) \= 0		{While constraints are violated}
  P(h,g,r) =penalty function		{Construct chosen penalty function}
  phi = f + P
  d = gradient(phi) @ x_i
  x_new = x_i + alpha*d			{Write x_new symbolically}
  phi = phi(x_new)			{Plug symbolic representation into phi}
  alpha* = solve(diff(phi) = 0)		{Solve to minimize pseudo-objective function}
  x_new = x_new(alpha*)			{Update design variables}
  r_new = C*r_i				{Update penalty parameter}
end
$$
```

### Dynamic Penalty Function
Unlike static functions, these incorporate a dynamic element that changes the penalty applied based on the infeasibility of the current solution. It also incorporates the previously mentioned penalty parameter, *p*. The primary benefit of this type of function is that it allows highly infeasible solutions at the start of the search process, but as time goes on, higher penalties are applied for solutions that fail to approach the feasible region [11].

One example of a dynamic penalty function is the popular Quadratic Penalty Function. The following shows the application of the Quadratic Function with *m* inequality constraints and *n* equality constraints [1,13]:

$$
\begin{align}
P = r \sum_{i=1}^m \delta_i^2 + r \sum_{j=1}^n g_j(x)^2 \\
\delta_i = h(x) \ if \ constraint \ i \ is \ violated \\
\delta_i = 0 \ if \ constraint \ i \ is \ satisfied \\
\end{align}
$$

If we restrict the problem to only inequality constraints, we can also describe the convergence of the Quadratic Penalty Method:

> *Theorem:* Let $x_k$ be a sequence generated by the quadratic exterior penalty method. Then, any limit point of the sequence is a solution to the inequality constrained minimization problem  [14].

Note that this indicates the final solution will satisfy the constraints as the number of iterations approaches infinity, not that it will necessarily be the optimum solution available on the edge of the feasible region. Commenting on the optimality of the limit point is more challenging, and requires additional assumptions about the chosen tolerances and penalty parameters for the given problem [13]. We can additionally note that conditioning of this problem is given by the Hessian of $\phi$. This provides the needed justification for the use of an increasingly large penalty parameter in order to avoid an ill-conditioned problem.

### Adaptive Penalty Function
This type of penalty function incorporates even more information about the iterative process, using knowledge of the ongoing success of the solver. The search for an optimal solution can be divided into intervals over $N_f$ generations. Whether or not the best solution was found in the previous interval informs the value of the next penalty function multiplier, $\lambda$, using constants, $\beta_1$ & $\beta_2$, guiding the search towards attractive regions or away from areas that have already been searched for an optimum [11].

$$
\begin{align}
P = \sum_{i=1}^m \lambda_k d_i^k \\
\lambda_{i+1} = \lambda_k \beta_1 \ if \ previous \ interval \ has \ infeasible \ best \ solution\\
\lambda_{i+1} = \lambda_k / \beta_2 \ if \ previous \ interval \ has \ feasible \ best \ solution\\
\lambda_{i+1} = \lambda_k \ otherwise\\
\end{align}
$$


## References (APA)

1. XXXX
2. XXXX
3. XXXX
4. XXXX
5. XXXX
6. XXXX
7. XXXX
8. XXXX
9. XXXX
10. XXXX
11. XXXX
12. XXXX
13. XXXX
14. XXXX
